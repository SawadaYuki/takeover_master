#Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation

##Abstract

現在、多数のMPI実装が利用可能であり、それぞれが高性能コンピューティングのさまざまな側面を強調したり、特定の研究課題を解決することを意図しています。

その結果、無数の互換性のないMPI実装が行われます。これらはすべて個別にインストールする必要があり、その組み合わせはエンドユーザーに重大なロジスティック上の課題をもたらします。

LAM / MPI、LA-MPI、およびFT-MPIプロジェクトのコードベースから得られた経験の影響を受けた先行研究を基にして、

Open MPIは基本的にコンポーネントのコンセプトを中心としたまったく新しい、実稼働品質のMPI-2実装です。

**Open MPIは、以前はオープンソースで、実稼働品質のMPI実装では利用できなかった独自の新機能の組み合わせを提供します。**

そのコンポーネントアーキテクチャは、サードパーティの研究のための安定したプラットフォームと、独立したソフトウェアアドオンのランタイム構成を可能にします。

**このホワイトペーパーでは、Open MPIの目標、設計、実装の概要を紹介します。**

#1 intro

並列コンピュータ・アーキテクチャーの進化により、並列アプリケーション開発者とエンド・ユーザーの両方にとって新たな傾向と課題が生じました。

今日、数万のプロセッサで構成されるシステムが利用可能です。
数十万のプロセッサーシステムが今後数年間で期待されています。 モノリシックな高性能コンピュータは、より魅力的な価格/性能比のため、PCとワークステーションのクラスタに着実に置き換えられています。

しかし、このようなクラスタは、統合されていない環境を提供するため、以前のアーキテクチャとは異なる（しばしば劣る）I / O動作をします。

グリッドおよびメタコンピューティングの努力により、並列アプリケーションで使用可能なプロセッサの数がさらに増加し、計算要素間の物理的距離も増加します。

**これらの傾向は、MPI実装の新たな課題につながります。
何千ものプロセッサを使用するMPIアプリケーションは、並列アプリケーションの全体的なパフォーマンスに劇的に影響するスケーラビリティの問題に直面しています。**

そのような問題には、プロセス制御、リソース枯渇、レイテンシの認識と管理、フォールトトレランス、一般的な通信パターンの最適化された集団操作が含まれます（ただしこれに限定されません）。

**ネットワーク層の伝送エラー（中程度のサイズのクラスターにとってはほとんど不可能と考えられていた）は、大規模な計算を扱うときは無視できません[4]。**

**さらに、実行中に並列アプリケーションにプロセス障害が発生する確率は、使用するプロセッサの数によって増加します。**

アプリケーションが最初から再起動しなくてもプロセスの失敗から生き残る場合は、定期的にチェックポイントファイルを書き込むか（最後の一貫したチェックポイント**[1、8]**からアプリケーションを再起動するか）、アプリケーション自体が適応的に処理できる必要があります 実行時のプロセス障害[3]。

**※[1] => MPICH-V:Toward a scalable fault tolerant MPI for volatile nodes.**

**これらの問題はすべて、現在の関連する研究課題です。** 実際に、いくつかのプロジェクトはさまざまなレベルで対応されています。

**しかし、MPI実装は、現在、それらのすべてを包括的に扱うことはできません。**

#1.1 Goals of Open MPI
参加しているすべての機関がMPIの実装に大きな経験を持っていますが、**Open MPIはLAM / MPI、LA-MPI、FT-MPIの単純な合併以上のものです。**

**以前のコードベースの影響を受けていましたが、Open MPIはMessage Passing Interfaceの全く新しい実装です。**

**MPI-1.2 [6]とMPI-2 [7]仕様を完全に実装し、同時マルチスレッドアプリケーション（MPI THREAD MULTIPLE）を完全にサポートしています。**

幅広い並列マシンを効率的にサポートするため、現在確立されているすべてのインターコネクトの高性能「ドライバ」が現在開発されています。

**これには、TCP / IP、共有メモリ、Myrinet、Quadrics、およびInfinibandが含まれます。**

Open MPIは、アプリケーションへの達成可能な帯域幅を最大化することができ、ノードに複数のネットワークインターフェイスが装備されている場合にネットワークデバイスの損失を動的に処理する能力を提供します。

したがって、ネットワークフェイルオーバーの処理はアプリケーションにとって完全に透過的です。
**※フェイルオーバーとは、稼働中のシステムに障害が発生した際に、代替システムがその機能を自動的に引き継ぎ、処理を続行する仕組み。**
**※トランスペアレントとは、透明な、透過的な、透き通った、などの意味を持つ英単語。
コンピュータの画面上に複数の要素が重なりあって表示されているとき、手前の要素に背後の要素の画像や色がうっすら重なって見える状態**

Open MPIの実行時環境は、対話型および非対話型の環境で並列アプリケーションを開始および管理するための基本的なサービスを提供します。
可能な場合は、既存のランタイム環境を活用して必要なサービスを提供します。
そのようなサービスが利用可能でない場合には、ユーザレベルのデーモンに基づくポータブル実行時環境が使用されます。

#4 Performance Results
Open MPIのポイントツーポイントメソドロジと他のパブリックMPIライブラリとの性能比較は、[11]にあります。

  本書のOpen MPIのパフォーマンスのサンプルとして、開発コードのスナップショットを使用して、MPI BcastおよびMPI AlltoallのPallasベンチマーク（v2.2.1）を実行しました。

Open MPIの基本COLLモジュールでこれらの関数に使用されたアルゴリズムは、モノリシックMPI実装（つまりコンポーネントに基づかない）であるLAM / MPI v6.5.9の対応する実装から導かれました。

集合演算は、データ移動のためにMPIのポイント・ツー・ポイント・メッセージ・パッシングを使用する標準線形/対数アルゴリズムに基づいています。

**Open MPIのコードはまだ完成していませんが、モノリシックアーキテクチャの同じアルゴリズムに対してその性能を測定することで、基本的な比較が可能になり、設計と実装が確実に確実になります。**

性能測定は、高速イーサネットを介して接続された2.4GHzデュアルプロセッサIntel Xeonマシンのクラスタで実行されました。

**図2は、Open MPIアプローチを使用する集合的な操作のパフォーマンスが、LAM / MPIの対応するものに大きなメッセージサイズで同じであることを示しています。**

短いメッセージの場合、現在、LAM / MPIと比較してOpen MPIに若干のオーバーヘッドがあります。

これは、Open MPIにまだ含まれていないLAM / MPIのポイントツーポイントレイテンシの最適化によるものです。 こ
れらの最適化はOpen MPIのリリースに含まれます。
しかし、グラフは、設計と全体的なアプローチが妥当であり、単純に最適化が必要であることを示しています。

#5 Summary

Open MPIは、MPI標準の新しい実装です。

**MPI-2のすべてのサポート、複数の同時ユーザースレッド、およびプロセスとネットワークの障害を処理するための複数のオプションを含む、本番環境品質の単一の実装で以前は利用できなかった機能を提供します。**

Open MPIグループはさらに、プロジェクトにソースコードを提供する第三者の開発者を扇動する適切な法的枠組みの確立に取り組んでいます。
Open MPIの最初の完全リリースは、2004年のスーパーコンピューティング会議で計画されています。

記載されている機能のほとんどとネットワークデバイスドライバ（tcp、shmem、およびループバックデバイス）の初期サブセットをサポートする初期ベータリリースは、2004年中頃リリース予定です。